# AI Load Balancer Client (Laptop)

## What This Does
This is the **client program** that runs on your laptops. It connects to the Raspberry Pi server, receives agricultural questions, processes them using local LLM models, and sends back responses.

## How to Run
```bash
cd Client_v1
python ai_load_balancer_client.py --server YOUR_RASPBERRY_PI_IP:50051
```

Example:
```bash
python ai_load_balancer_client.py --server 192.168.1.32:50051
```

## What Happens Step by Step

### 1. **Client Startup**
- ğŸš€ "AI Load Balancer Client v1.0 initialized"
- ğŸ†” Generates unique client ID (e.g., client-laptop-001)
- ğŸŒ Shows server address it will connect to

### 2. **System Analysis**
- ğŸ–¥ï¸ Scans your laptop's hardware:
  - **CPU**: Cores and frequency
  - **RAM**: Total memory
  - **GPU**: Graphics card and memory
  - **Performance Score**: Calculated based on specs
- ğŸ“Š Shows: "AI Performance Score: 85.5"

### 3. **Server Connection**
- ğŸ”Œ "Connecting to AI Load Balancer at 192.168.1.32:50051"
- âœ… "Connected to AI Load Balancer successfully"
- ğŸ“¡ Detects your laptop's local IP address

### 4. **Registration**
- ğŸ“ Sends your laptop specs to Raspberry Pi server
- âœ… "Successfully registered as AI client"
- ğŸ¯ Server assigns LLM model based on your performance:
  - **High specs**: Gets 8B model (best quality)
  - **Medium specs**: Gets 3B model (good quality)
  - **Low specs**: Gets 1B model (fast response)

### 5. **LLM Model Setup**
- ğŸ¤– "Assigned LLM model: dhenu2-llama3.2-3b"
- ğŸŒ "Port: 7862"
- ğŸš€ Starts local LLM server on assigned port
- âœ… "Mock LLM server is ready!"

### 6. **Ready State**
- ğŸ’“ Starts heartbeat (keeps connection alive)
- ğŸŒ Starts gRPC server to receive requests
- âœ… "AI Client ready for distributed LLM processing"
- â³ Waits for questions from Raspberry Pi

### 7. **Processing Requests** (when server sends questions)
- ğŸ“¥ "Received AI request: what is fertilizer?"
- ğŸ¯ Shows assigned model and full prompt
- ğŸ“Š Shows your system specs again

### 8. **LLM Processing**
- ğŸ”„ "Starting LLM processing with dhenu2-llama3.2-3b"
- ğŸ“¡ Calls local LLM server on assigned port
- ğŸ¤– LLM generates agricultural response
- âœ… "LLM processing completed"

### 9. **Response Delivery**
- ğŸ“¤ Sends response back to Raspberry Pi server
- â±ï¸ Shows processing time (e.g., "3.2 seconds")
- âœ… "Response sent successfully"

### 10. **Continuous Operation**
- ğŸ”„ Stays connected and ready for more questions
- ğŸ’“ Sends periodic heartbeats to server
- ğŸ“± Processes multiple requests as they come

## What Your Laptop Contributes
- **Processing Power**: Your laptop's CPU/GPU processes the LLM
- **Model Quality**: Better specs = better model = higher quality responses
- **Parallel Processing**: Multiple laptops work simultaneously
- **Load Distribution**: Shares the computational workload

## Logs You'll See
```
[CLIENT DATA FLOW] RECEIVED AI REQUEST
[CLIENT DATA FLOW] System Performance Score: 85.5
[DATA FLOW] Starting LLM processing
[DATA FLOW] Calling LLM server at localhost:7862
[DATA FLOW] LLM generated response
[CLIENT DATA FLOW] LLM PROCESSING COMPLETED
```

## Requirements
- Python 3.7+
- gRPC libraries
- gradio_client library
- Network connection to Raspberry Pi

## Your Role in the System
1. **Connect**: Your laptop joins the distributed network
2. **Contribute**: Your hardware processes AI requests
3. **Collaborate**: Works with other laptops for better results
4. **Respond**: Sends agricultural advice back to users

The client runs automatically once started - it will process requests and contribute to the distributed AI system! ğŸš€